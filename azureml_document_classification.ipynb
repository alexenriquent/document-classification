{
  "cells": [
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import azureml.core\nfrom azureml.core import Workspace\n\nws = Workspace.get(name=\"MachineLearning\",\n               subscription_id='41e7097b-b33b-4dad-ba38-6a953141427e',\n               resource_group='MachineLearning')\n\nprint('Ready to use Azure ML {} to work with {}'.format(azureml.core.VERSION, ws.name))",
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Ready to use Azure ML 1.0.72 to work with MachineLearning\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import os\nfrom azureml.core import Datastore\n\nif 'mldatastore'not in ws.datastores:\n    blob_datastore_name='mldatastore' # Name of the datastore to workspace\n    container_name=os.getenv(\"BLOB_CONTAINER\", \"mldata\") # Name of Azure blob container\n    account_name=os.getenv(\"BLOB_ACCOUNTNAME\", \"alexmlstorage\") # Storage account name\n    account_key=os.getenv(\"BLOB_ACCOUNT_KEY\", \"HNHyr3lkKhNfcAMDGuv9K3TvuysoOKjlkzP2dKGFJc89sKixfNxix9uQp9w4HtYIOV0ziZLHy2asI+zxkesUcg==\") # Storage account key\n    \n    try:\n        blob_ds = Datastore.register_azure_blob_container(workspace=ws, \n                                                                 datastore_name=blob_datastore_name, \n                                                                 container_name=container_name, \n                                                                 account_name=account_name,\n                                                                 account_key=account_key)\n        print('Datastore registered.')\n    except Exception as ex:\n        print(ex)\nelse:\n    blob_ds = Datastore.get(ws, datastore_name='mldatastore')\n    print('Datastore already registered.')",
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Datastore already registered.\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core import Dataset\n\nif 'document_files' not in ws.datasets:\n    #Create a file dataset from the path on the datastore.\n    document_ds = Dataset.File.from_files(path=(blob_ds, ''))\n\n    # Register the file dataset\n    try:\n        document_ds = document_ds.register(workspace=ws, \n                                            name='document_files',\n                                            description='documents',\n                                            create_new_version=True)\n        print('Dataset registered.')\n    except Exception as ex:\n        print(ex)\nelse:\n    document_ds = Dataset.get_by_name(ws, 'document_files')\n    print('Dataset already registered.')",
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Dataset already registered.\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core.compute import ComputeTarget, AmlCompute\nfrom azureml.core.compute_target import ComputeTargetException\n\ncluster_name = 'aml-cluster'\n\ntry:\n    # Check for existing compute target\n    training_cluster = ComputeTarget(workspace=ws, name=cluster_name)\n    print('Found existing cluster, use it.')\nexcept ComputeTargetException:\n    # If it doesn't already exist, create it\n    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS12_V2', max_nodes=4)\n    training_cluster = ComputeTarget.create(ws, cluster_name, compute_config)\n\ntraining_cluster.wait_for_completion(show_output=True)",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Found existing cluster, use it.\nSucceeded\nAmlCompute wait for completion finished\nMinimum number of nodes requested have been provisioned\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "experiment_folder = 'experiment'\nos.makedirs(experiment_folder, exist_ok=True)\n\nprint(experiment_folder, 'folder created')",
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": "experiment folder created\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%%writefile $experiment_folder/train.py\n\nimport argparse\nimport numpy as np\nimport pandas as pd\nimport re\nimport pickle\nimport tika\nimport glob\nfrom keras.layers import  Dropout, Dense\nfrom keras.models import Sequential\nfrom keras.utils import np_utils\nfrom keras.models import load_model\nfrom sklearn import metrics\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom tika import parser as tika_parser\nfrom azureml.core import Run\nfrom azureml.core import Experiment\nfrom azureml.core import Model\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--data-folder', type=str, dest='data_folder', help='data folder mounting point')\nargs = parser.parse_args()\n\ndata_folder = args.data_folder\nprint('Data folder:', data_folder)\n\nrun = Run.get_context()\n\nprint('Loading Data...')\nagenda_paths = glob.glob(data_folder + \"**/Agendas/*\")\nmedicalrecord_paths = glob.glob(data_folder + \"**/MedicalRecords/*\")\npaper_paths = glob.glob(data_folder + \"**/Papers/*\")\nresume_paths = glob.glob(data_folder + \"**/Resumes/*\")\n\ndef preprocess_text(text):\n    processed = re.sub('[^a-zA-Z]', ' ', text)\n    processed = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', processed)\n    processed = re.sub(r'\\s+', ' ', processed)\n\n    return processed\n\ndef process_raw_data(paths, label):\n    data = {\n        'Content': [],\n        'Type': [label] * len(paths)\n    }\n    \n    for path in paths:\n        print('Processing {}'.format(path))\n        parsed = tika_parser.from_file(path)\n        text = preprocess_text(parsed[\"content\"])\n        data['Content'].append(text)\n    \n    return pd.DataFrame(data, columns = ['Content', 'Type'])\n\ndef tfidf(X_train, X_test, num_words=5000):\n\n    vectorizer_x = TfidfVectorizer(max_features=num_words)\n    X_train = vectorizer_x.fit_transform(X_train).toarray()\n    X_test = vectorizer_x.transform(X_test).toarray()\n\n    vectoriser_file = 'vectoriser.pkl'\n    pickle.dump(vectorizer_x, open(vectoriser_file,'wb'))\n    run.upload_file(name = 'models/' + vectoriser_file, path_or_stream = './' + vectoriser_file)\n    \n    print(\"tf-idf with\", str(np.array(X_train).shape[1]), \"features\")\n\n    return (X_train,X_test)\n\ndef build_DNN_model(shape, num_classes, dropout=0.2):\n\n    model = Sequential()\n    node = 512 # number of nodes\n    num_layers = 4 # number of  hidden layer\n    model.add(Dense(node,input_dim=shape,activation='relu'))\n    model.add(Dropout(dropout))\n\n    for i in range(0, num_layers):\n        model.add(Dense(node,input_dim=node,activation='relu'))\n        model.add(Dropout(dropout))\n\n    model.add(Dense(num_classes, activation='softmax'))\n    model.compile(loss='categorical_crossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy'])\n\n    return model\n\nagenda_df = process_raw_data(agenda_paths, 'agenda')\nmedicalrecord_df = process_raw_data(medicalrecord_paths, 'medicalrecord')\npaper_df = process_raw_data(paper_paths, 'paper')\nresume_df = process_raw_data(resume_paths, 'resume')\n\ndf = pd.concat([agenda_df, medicalrecord_df, paper_df, resume_df], axis=0)\n\nX = df['Content'].values\ny = df['Type'].values\n\nencoder = LabelEncoder()\nencoder.fit(y)\nencoded_y = encoder.transform(y)\ndummy_y = np_utils.to_categorical(encoded_y)\n\nencoder_file = 'encoder.pkl'\npickle.dump(encoder, open(encoder_file,'wb'))\nrun.upload_file(name = 'models/' + encoder_file, path_or_stream = './' + encoder_file)\n\nX_train, X_test, y_train, y_test = train_test_split(X, dummy_y, test_size=0.20, random_state=7)\nX_train_tfidf, X_test_tfidf = tfidf(X_train, X_test)\n\nmodel = build_DNN_model(X_train_tfidf.shape[1], 4)\n\nprint(model.summary())\n\nhistory = model.fit(X_train_tfidf, y_train, validation_split=0.2, epochs=5, batch_size=128, verbose=1)\nscore = model.evaluate(X_test_tfidf, y_test, verbose=1)\n\nprint(\"Test Score:\", score[0])\nprint(\"Test Accuracy:\", score[1])\n\nrun.log('Score', np.float(score[0]))\nrun.log('Accuracy', np.float(score[1]))\n\ndnn_file = 'dnn.h5'\nmodel.save(dnn_file) \nrun.upload_file(name = 'models/' + dnn_file, path_or_stream = './' + dnn_file)\n\nrun.complete()\n\nrun.register_model(model_path='./models', \n                   model_name='document_classification_model',\n                   description='Document classification model',\n                   properties={'Accuracy': run.get_metrics()['Accuracy']})\n\nprint('Model trained and registered.')",
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Overwriting experiment/train.py\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core.environment import Environment\nfrom azureml.core.conda_dependencies import CondaDependencies\n\nenv = Environment('document-classification')\n\nenv.docker.enabled = True\nenv.docker.base_image = \"intelmpi:v1\"\nenv.docker.base_image_registry.address = \"machinelearndcc81c93.azurecr.io\"\nenv.docker.base_image_registry.username = \"machinelearndcc81c93\"\nenv.docker.base_image_registry.password = \"IczI34/hZD+M3QAw2cDyHzR7iJRMEdAj\"\n\n# to install required packages\nconda_packages = ['numpy', 'scikit-learn']\npip_packages = ['azureml-dataprep[pandas,fuse]', 'azureml-defaults', 'tensorflow', 'keras', 'tika']\n\ncd = CondaDependencies.create(pip_packages=pip_packages, conda_packages=conda_packages)\nenv.python.conda_dependencies = cd\n    \n# Register environment to re-use later\nenv.register(workspace = ws)\nprint('Environment registered.')",
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Environment registered.\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.train.estimator import Estimator\nfrom azureml.core import Experiment\nfrom azureml.widgets import RunDetails\n\n# Set the script parameters\nscript_params = {\n    '--data-folder': document_ds.as_named_input('documents').as_mount()\n}\n\n# Get the training dataset\ndiabetes_ds = ws.datasets.get(\"document_files\")\n\n# Create an estimator\nestimator = Estimator(source_directory=experiment_folder,\n                       entry_script='train.py',\n                       script_params=script_params,\n                       compute_target=cluster_name,\n                       environment_definition=env\n                      )\n\n# Create an experiment\nexperiment_name = 'documentclassification-training'\nexperiment = Experiment(workspace = ws, name = experiment_name)\n\n# Run the experiment\nrun = experiment.submit(config=estimator)\n\n# Show the run details while running\nRunDetails(run).show()\nrun.wait_for_completion()",
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fca51f2e4b0b4750955c6edb8ccedceb",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', '…"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/aml.mini.widget.v1": "{\"status\": \"Running\", \"workbench_run_details_uri\": \"https://ml.azure.com/experiments/documentclassification-training/runs/documentclassification-training_1586586326_a07bcf03?wsid=/subscriptions/41e7097b-b33b-4dad-ba38-6a953141427e/resourcegroups/MachineLearning/workspaces/MachineLearning\", \"run_id\": \"documentclassification-training_1586586326_a07bcf03\", \"run_properties\": {\"run_id\": \"documentclassification-training_1586586326_a07bcf03\", \"created_utc\": \"2020-04-11T06:25:38.214289Z\", \"properties\": {\"_azureml.ComputeTargetType\": \"amlcompute\", \"ContentSnapshotId\": \"1b8ba5d4-b1c0-49eb-8ff0-832bccc973ce\", \"AzureML.DerivedImageName\": \"azureml/azureml_13204f7f39d612b4bfd4cf1d9fa89800\", \"ProcessInfoFile\": \"azureml-logs/process_info.json\", \"ProcessStatusFile\": \"azureml-logs/process_status.json\"}, \"tags\": {\"_aml_system_ComputeTargetStatus\": \"{\\\"AllocationState\\\":\\\"steady\\\",\\\"PreparingNodeCount\\\":0,\\\"RunningNodeCount\\\":0,\\\"CurrentNodeCount\\\":0}\"}, \"script_name\": null, \"arguments\": null, \"end_time_utc\": null, \"status\": \"Running\", \"log_files\": {\"azureml-logs/55_azureml-execution-tvmps_0acf047066a72a9022000013c117b570088bfc93b9bc350f33948147bd43a5ae_d.txt\": \"https://machinelearnin7708334383.blob.core.windows.net/azureml/ExperimentRun/dcid.documentclassification-training_1586586326_a07bcf03/azureml-logs/55_azureml-execution-tvmps_0acf047066a72a9022000013c117b570088bfc93b9bc350f33948147bd43a5ae_d.txt?sv=2019-02-02&sr=b&sig=5cg2xzniovhfuhchlACPQg4Ev3mTGRodf95h9S9B0AI%3D&st=2020-04-11T06%3A20%3A44Z&se=2020-04-11T14%3A30%3A44Z&sp=r\"}, \"log_groups\": [[\"azureml-logs/55_azureml-execution-tvmps_0acf047066a72a9022000013c117b570088bfc93b9bc350f33948147bd43a5ae_d.txt\"]], \"run_duration\": \"0:05:03\"}, \"child_runs\": [], \"children_metrics\": {}, \"run_metrics\": [], \"run_logs\": \"2020-04-11T06:29:52Z Starting output-watcher...\\n2020-04-11T06:29:52Z IsDedicatedCompute == True, won't poll for Low Pri Preemption\\nLogin Succeeded\\nUsing default tag: latest\\nlatest: Pulling from azureml/azureml_13204f7f39d612b4bfd4cf1d9fa89800\\nfe703b657a32: Pulling fs layer\\nf9df1fafd224: Pulling fs layer\\na645a4b887f9: Pulling fs layer\\n57db7fe0b522: Pulling fs layer\\n489c184a3ffd: Pulling fs layer\\n2f1d0788913d: Pulling fs layer\\n9adaf8fba06e: Pulling fs layer\\n32c8ffcdd900: Pulling fs layer\\n4ba50e238086: Pulling fs layer\\n1ad9a6b50574: Pulling fs layer\\n11193a8b5b81: Pulling fs layer\\n13e8cf02786b: Pulling fs layer\\n744853fbb531: Pulling fs layer\\n5603221065b6: Pulling fs layer\\nf615701f82d4: Pulling fs layer\\n32c8ffcdd900: Waiting\\n4ba50e238086: Waiting\\n489c184a3ffd: Waiting\\n1ad9a6b50574: Waiting\\n11193a8b5b81: Waiting\\n2f1d0788913d: Waiting\\n5603221065b6: Waiting\\n13e8cf02786b: Waiting\\nf615701f82d4: Waiting\\n744853fbb531: Waiting\\n9adaf8fba06e: Waiting\\n57db7fe0b522: Waiting\\nf9df1fafd224: Verifying Checksum\\nf9df1fafd224: Download complete\\n57db7fe0b522: Verifying Checksum\\n57db7fe0b522: Download complete\\nfe703b657a32: Verifying Checksum\\nfe703b657a32: Download complete\\n489c184a3ffd: Verifying Checksum\\n489c184a3ffd: Download complete\\n2f1d0788913d: Verifying Checksum\\n2f1d0788913d: Download complete\\n4ba50e238086: Download complete\\n1ad9a6b50574: Verifying Checksum\\n1ad9a6b50574: Download complete\\n9adaf8fba06e: Verifying Checksum\\n9adaf8fba06e: Download complete\\n11193a8b5b81: Verifying Checksum\\n11193a8b5b81: Download complete\\n13e8cf02786b: Verifying Checksum\\n13e8cf02786b: Download complete\\n744853fbb531: Download complete\\nf615701f82d4: Verifying Checksum\\n32c8ffcdd900: Verifying Checksum\\n32c8ffcdd900: Download complete\\nfe703b657a32: Pull complete\\nf9df1fafd224: Pull complete\\na645a4b887f9: Pull complete\\n57db7fe0b522: Pull complete\\n489c184a3ffd: Pull complete\\n5603221065b6: Verifying Checksum\\n5603221065b6: Download complete\\n2f1d0788913d: Pull complete\\n9adaf8fba06e: Pull complete\\n32c8ffcdd900: Pull complete\\n4ba50e238086: Pull complete\\n1ad9a6b50574: Pull complete\\n11193a8b5b81: Pull complete\\n13e8cf02786b: Pull complete\\n744853fbb531: Pull complete\\n\", \"graph\": {}, \"widget_settings\": {\"childWidgetDisplay\": \"popup\", \"send_telemetry\": false, \"log_level\": \"INFO\", \"sdk_version\": \"1.0.72\"}, \"loading\": false}"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 81,
          "data": {
            "text/plain": "{'runId': 'documentclassification-training_1586586326_a07bcf03',\n 'target': 'aml-cluster',\n 'status': 'Finalizing',\n 'startTimeUtc': '2020-04-11T06:29:48.994886Z',\n 'properties': {'_azureml.ComputeTargetType': 'amlcompute',\n  'ContentSnapshotId': '1b8ba5d4-b1c0-49eb-8ff0-832bccc973ce',\n  'AzureML.DerivedImageName': 'azureml/azureml_13204f7f39d612b4bfd4cf1d9fa89800',\n  'ProcessInfoFile': 'azureml-logs/process_info.json',\n  'ProcessStatusFile': 'azureml-logs/process_status.json'},\n 'inputDatasets': [{'dataset': {'id': '0d2fdb57-6235-4f6a-9c07-32593b9df03d'}, 'consumptionDetails': {'type': 'RunInput', 'inputName': 'documents', 'mechanism': 'Mount'}}],\n 'runDefinition': {'script': 'train.py',\n  'useAbsolutePath': False,\n  'arguments': ['--data-folder', 'DatasetConsumptionConfig:documents'],\n  'sourceDirectoryDataStore': None,\n  'framework': 'Python',\n  'communicator': 'None',\n  'target': 'aml-cluster',\n  'dataReferences': {},\n  'data': {'documents': {'dataLocation': {'dataset': {'id': '0d2fdb57-6235-4f6a-9c07-32593b9df03d'},\n     'dataPath': None},\n    'createOutputDirectories': False,\n    'mechanism': 'Mount',\n    'environmentVariableName': 'documents',\n    'pathOnCompute': None,\n    'overwrite': False}},\n  'jobName': None,\n  'maxRunDurationSeconds': None,\n  'nodeCount': 1,\n  'environment': {'name': 'document-classification',\n   'version': '10',\n   'python': {'interpreterPath': 'python',\n    'userManagedDependencies': False,\n    'condaDependencies': {'channels': ['conda-forge'],\n     'dependencies': ['python=3.6.2',\n      {'pip': ['azureml-dataprep[pandas,fuse]',\n        'azureml-defaults==1.0.72.*',\n        'tensorflow',\n        'keras',\n        'tika']},\n      'numpy',\n      'scikit-learn'],\n     'name': 'azureml_912ca7bbac4f0ab7a103e2c81943a708'},\n    'baseCondaEnvironment': None},\n   'environmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE'},\n   'docker': {'baseImage': 'intelmpi:v1',\n    'baseDockerfile': None,\n    'baseImageRegistry': {'address': 'machinelearndcc81c93.azurecr.io',\n     'username': 'machinelearndcc81c93',\n     'password': 'AzureMlSecret=documentclassification-training_1586586326_a07bcf03#RunConfiguration#ContainerRegistry#Password'},\n    'enabled': True,\n    'arguments': []},\n   'spark': {'repositories': [], 'packages': [], 'precachePackages': True},\n   'inferencingStackVersion': None},\n  'history': {'outputCollection': True,\n   'directoriesToWatch': ['logs'],\n   'snapshotProject': True},\n  'spark': {'configuration': {'spark.app.name': 'Azure ML Experiment',\n    'spark.yarn.maxAppAttempts': '1'}},\n  'amlCompute': {'name': None,\n   'vmSize': None,\n   'retainCluster': False,\n   'clusterMaxNodeCount': 1},\n  'tensorflow': {'workerCount': 1, 'parameterServerCount': 1},\n  'mpi': {'processCountPerNode': 1},\n  'hdi': {'yarnDeployMode': 'Cluster'},\n  'containerInstance': {'region': None, 'cpuCores': 2, 'memoryGb': 3.5},\n  'exposedPorts': None,\n  'docker': {'useDocker': True,\n   'sharedVolumes': True,\n   'shmSize': '2g',\n   'arguments': []},\n  'cmk8sCompute': {'configuration': {}}},\n 'logFiles': {'azureml-logs/55_azureml-execution-tvmps_0acf047066a72a9022000013c117b570088bfc93b9bc350f33948147bd43a5ae_d.txt': 'https://machinelearnin7708334383.blob.core.windows.net/azureml/ExperimentRun/dcid.documentclassification-training_1586586326_a07bcf03/azureml-logs/55_azureml-execution-tvmps_0acf047066a72a9022000013c117b570088bfc93b9bc350f33948147bd43a5ae_d.txt?sv=2019-02-02&sr=b&sig=BXlfFV%2Fc0of1d7xb3UuH4srqFlF6bvB3kM2qlD3DVrs%3D&st=2020-04-11T06%3A25%3A56Z&se=2020-04-11T14%3A35%3A56Z&sp=r',\n  'azureml-logs/65_job_prep-tvmps_0acf047066a72a9022000013c117b570088bfc93b9bc350f33948147bd43a5ae_d.txt': 'https://machinelearnin7708334383.blob.core.windows.net/azureml/ExperimentRun/dcid.documentclassification-training_1586586326_a07bcf03/azureml-logs/65_job_prep-tvmps_0acf047066a72a9022000013c117b570088bfc93b9bc350f33948147bd43a5ae_d.txt?sv=2019-02-02&sr=b&sig=1%2FKzw4fDUW9E6%2FqdRhiD5sOVIVkM4i891ghMlJDWIAM%3D&st=2020-04-11T06%3A25%3A56Z&se=2020-04-11T14%3A35%3A56Z&sp=r',\n  'azureml-logs/70_driver_log.txt': 'https://machinelearnin7708334383.blob.core.windows.net/azureml/ExperimentRun/dcid.documentclassification-training_1586586326_a07bcf03/azureml-logs/70_driver_log.txt?sv=2019-02-02&sr=b&sig=g3CDPY8%2Fq%2BB83qWxSYjhvnUTqj%2F1PN6gqgMr8R2YTk8%3D&st=2020-04-11T06%3A25%3A56Z&se=2020-04-11T14%3A35%3A56Z&sp=r',\n  'azureml-logs/75_job_post-tvmps_0acf047066a72a9022000013c117b570088bfc93b9bc350f33948147bd43a5ae_d.txt': 'https://machinelearnin7708334383.blob.core.windows.net/azureml/ExperimentRun/dcid.documentclassification-training_1586586326_a07bcf03/azureml-logs/75_job_post-tvmps_0acf047066a72a9022000013c117b570088bfc93b9bc350f33948147bd43a5ae_d.txt?sv=2019-02-02&sr=b&sig=4mtYlKYzDSk7ckMuwq7zeMOBk5u2J%2FdD0ocpgEph8Jw%3D&st=2020-04-11T06%3A25%3A56Z&se=2020-04-11T14%3A35%3A56Z&sp=r',\n  'azureml-logs/process_info.json': 'https://machinelearnin7708334383.blob.core.windows.net/azureml/ExperimentRun/dcid.documentclassification-training_1586586326_a07bcf03/azureml-logs/process_info.json?sv=2019-02-02&sr=b&sig=WpwKfElMoQoBqAXQWsZkBAIyMjEe1zHupxWsT0OkKYw%3D&st=2020-04-11T06%3A25%3A56Z&se=2020-04-11T14%3A35%3A56Z&sp=r',\n  'azureml-logs/process_status.json': 'https://machinelearnin7708334383.blob.core.windows.net/azureml/ExperimentRun/dcid.documentclassification-training_1586586326_a07bcf03/azureml-logs/process_status.json?sv=2019-02-02&sr=b&sig=C5T52tCG6gXVDqR2%2BONQbDb7R8Y4vR3VrK1DAYmz9y0%3D&st=2020-04-11T06%3A25%3A56Z&se=2020-04-11T14%3A35%3A56Z&sp=r'}}"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import os\n\nservice_folder = 'service'\n\n# Create a folder for the web service files\nexperiment_folder = './' + service_folder\nos.makedirs(service_folder, exist_ok=True)\n\nprint(service_folder, 'folder created.')",
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": "service folder created.\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%%writefile $service_folder/score.py\nimport json\nimport pickle\nimport numpy as np\nimport re\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.models import load_model\nfrom azureml.core.model import Model\n\n# Called when the service is loaded\ndef init():\n    global model\n    global vectoriser\n    global encoder\n    \n    # Get the path where the deployed model can be found.\n    model_path = Model.get_model_path('document_classification_model')\n    \n    # load models\n    model = load_model(model_path + '/dnn.h5')\n\n    with open(model_path + '/vectoriser.pkl','rb') as handle:\n        vectoriser = pickle.load(handle)\n\n    with open(model_path + '/encoder.pkl','rb') as handle:\n        encoder = pickle.load(handle)\n\n# Called when a request is received\ndef run(data):\n    try:\n        # Pick out the text property of the JSON request.\n        # This expects a request in the form of {\"text\": \"some text to classify\"}\n        data = json.loads(data)\n        text = preprocess_text(data['text'])\n        vector = vectoriser.transform([text]).toarray()\n        prediction = model.predict([vector])\n        prediction = np.round(prediction[0])\n        prediction = encoder.inverse_transform(np.where(prediction == 1))\n        return prediction[0]\n    except Exception as e:\n        error = str(e)\n        return error\n\ndef preprocess_text(text):\n    processed = re.sub('[^a-zA-Z]', ' ', text)\n    processed = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', processed)\n    processed = re.sub(r'\\s+', ' ', processed)\n\n    return processed",
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Overwriting service/score.py\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core.environment import Environment\nfrom azureml.core.conda_dependencies import CondaDependencies\n\n# Create the environment\ndeployment_env = Environment('document-classification-deployment')\n\n# to install required packages\nconda_packages = ['numpy', 'scikit-learn']\npip_packages = ['azureml-dataprep[pandas,fuse]', 'azureml-defaults', 'tensorflow', 'keras']\n\nconda_dep = CondaDependencies.create(pip_packages=pip_packages, conda_packages=conda_packages)\n\n# Adds dependencies to PythonSection of myenv\ndeployment_env.python.conda_dependencies=conda_dep",
      "execution_count": 113,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core import Model\nfrom azureml.core.webservice import AciWebservice\nfrom azureml.core.model import InferenceConfig\n\nmodel = ws.models['document_classification_model']\n\ninference_config = InferenceConfig(source_directory=service_folder, \n                                   entry_script='score.py',\n                                   environment=deployment_env)\n\ndeployment_config = AciWebservice.deploy_configuration(cpu_cores = 1, memory_gb = 1)\n\nservice_name = \"document-classification-service\"\n\nservice = Model.deploy(ws, service_name, [model], inference_config, deployment_config)\n\nservice.wait_for_deployment(True)\nprint(service.state)",
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Running....................................................\nSucceeded\nACI service creation operation finished, operation \"Succeeded\"\nHealthy\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import requests\nimport json\n\nscoring_uri = 'http://1400a94a-0b41-4f9a-955a-6154add77094.westus.azurecontainer.io/score'\nheaders = {'Content-Type':'application/json'}\n\ntest_data = json.dumps({'text': 'AMY PROFILE Fund accountant with nearly 2 years of experience in hedge fund administration, which includes preparation of NAV calculations, financial statements and associated reports. Consistently meeting deadlines while ensuring a high quality of work standards. Fast learner, driven for results and analytical in problem solving. WORK EXPERIENCE Citco Fund Services (Singapore) Pte Ltd  Jan 2016 – Present Fund Accountant Calculation of estimate and final NAVs on a daily, weekly and monthly basis Preparation of cash and position reconciliation reports  Daily pre-production tasks such as price checks, interest accruals and fees booking  Maintaining day-to-day relationships with investment managers, brokers and auditors  Communicating with the reconciliations team to ensure consistent and high-quality standards when delivering NAV packages  Undertaking fund migrations from Citco Toronto and Citco Dublin Office  Familiar with various pricing valuation models e.g. Independent Price Verification, External Valuer  Investigation and resolution of breaks  Training of new fund accountants in the team  Assist in reviewing estimate and final NAV packs Norgas Carriers Private Limited Nov 2014 – Apr 2015 Accountant  Processing invoices from suppliers and scheduling payment remittances  Assisted in collecting outstanding debts from debtors  Involved in the year end closing of accounts  Liaised with external auditors during interim and final audit  Monthly bank reconciliation  Perform intercompany journal entries and reconciliation Singapore Armed Forces Jul 2013 – July 2014 Operation Specialist/ NSF Battery Sergeant Major (2nd Sergeant, 24th Battalion Singapore Artillery)  In charge of the training management and administration matters, ensuring smooth running and operation of the battalion  Responsible for the welfare, discipline and regimentation of 60 fellow NSFs  Rallied with superiors to keep a lookout for troubled soldiers, acted as the first level of feedback for peers EDUCATION AND QUALIFICATIONS Royal Melbourne Institute of Technology (RMIT) Jul 2014 – Oct 2015 Bachelor of Business (Accountancy) GPA: 3.1 / 4 Temasek Polytechnic Apr 2009 – Apr 2012 Diploma in Accounting and Finance Ngee Ann Secondary School Jan 2005 – Nov 2008 GCE ‘O’ Levels CertificateSKILLS AND COMPETENCIES  Trained in accounting softwares, Aexeo, Agresso Business World, Sage AccPac and MYOB  Proficient in Microsoft Excel, PowerPoint and Word  Competent in Financial and Economic Databases (Bloomberg, Thomson Reuters)  Languages spoken: English, Chinese, Cantonese  Languages written: English, Chinese'})\n\nresponse = requests.post(scoring_uri, data=test_data, headers=headers)\n\nprint(response.status_code)\nprint(response.elapsed)\nprint(response.json())",
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": "200\n0:00:00.097396\nresume\n",
          "name": "stdout"
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}