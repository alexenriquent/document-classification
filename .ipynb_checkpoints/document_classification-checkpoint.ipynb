{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Classification\n",
    "This notebook presents an end-to-end process for document classification using a deep feedforward neural network. The labels to predict include \"agenda\", \"medical record\", \"paper\" and \"resume\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "Import all the libraries necessary for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import  Dropout, Dense\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "from keras.models import load_model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn import metrics\n",
    "import pickle\n",
    "import tika\n",
    "import glob\n",
    "from tika import parser as tika_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "The raw dataset consists of four types of documents, including agendas, medical records, academic papers and resumes. It is worth noting that the data is imbalanced as the dataset only contains 100 agenda examples where other document types have 500 examples each.\n",
    "\n",
    "Retrieve all the file paths for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"C:\\\\Users\\\\Alex.Chokwijitkul\\\\Desktop\\\\Document Classification\\\\data\\\\\"\n",
    "\n",
    "agenda_paths = glob.glob(root_path + \"Agendas/*\")\n",
    "medicalrecord_paths = glob.glob(root_path + \"MedicalRecords/*\")\n",
    "paper_paths = glob.glob(root_path + \"Papers/*\")\n",
    "resume_paths = glob.glob(root_path + \"Resumes/*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "Add functions for reading raw data from files, cleaning and tranforming it into a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    processed = re.sub('[^a-zA-Z]', ' ', text)\n",
    "    processed = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', processed)\n",
    "    processed = re.sub(r'\\s+', ' ', processed)\n",
    "\n",
    "    return processed\n",
    "\n",
    "def process_raw_data(paths, label):\n",
    "    data = {\n",
    "        'Content': [],\n",
    "        'Type': [label] * len(paths)\n",
    "    }\n",
    "    \n",
    "    for path in paths:\n",
    "        print('Processing {}'.format(path))\n",
    "        parsed = tika_parser.from_file(path)\n",
    "        text = preprocess_text(parsed[\"content\"])\n",
    "        data['Content'].append(text)\n",
    "    \n",
    "    return pd.DataFrame(data, columns = ['Content', 'Type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process all the files and concatenate the results into a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agenda_df = process_raw_data(agenda_paths, 'agenda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medicalrecord_df = process_raw_data(medicalrecord_paths, 'medicalrecord')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_df = process_raw_data(paper_paths, 'paper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_df = process_raw_data(resume_paths, 'resume')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([agenda_df, medicalrecord_df, paper_df, resume_df], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "Term Frequency â€“ Inverse Document Frequency (tf-idf) is used to assign weightage to the feature vector.\n",
    "\n",
    "Simply use the `TfidfVectorizer` to fit and tranform the training data. Also, save the vectoriser object as an pickle file as it will be needed later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(X_train, X_test, num_words=5000):\n",
    "\n",
    "    vectorizer_x = TfidfVectorizer(max_features=num_words)\n",
    "    X_train = vectorizer_x.fit_transform(X_train).toarray()\n",
    "    X_test = vectorizer_x.transform(X_test).toarray()\n",
    "\n",
    "    pickle.dump(vectorizer_x, open('vectoriser.pkl','wb'))\n",
    "    print(\"tf-idf with\", str(np.array(X_train).shape[1]), \"features\")\n",
    "\n",
    "    return (X_train,X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "Add a function for creating and compileing a deep neural network. For this project, we will use a DNN with 4 hidden layers, each layer contains 512 hidden neurals with the rectified linear unit (ReLU) except for the last layer where the softmax activation function will be used. Dropout regularisation is added after each dense layer. The model will use the Adam optimiser and the categorical cross entropy loss function when training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_DNN_model(shape, num_classes, dropout=0.2):\n",
    "\n",
    "    model = Sequential()\n",
    "    node = 512 # number of nodes\n",
    "    num_layers = 4 # number of  hidden layer\n",
    "    model.add(Dense(node,input_dim=shape,activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    for i in range(0, num_layers):\n",
    "        model.add(Dense(node,input_dim=node,activation='relu'))\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Retrieve a series of raw text examples and their labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['Content'].values\n",
    "y = df['Type'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `LabelEncoder` to tranform each label text into a one-hot vector. Save the pickle file for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "encoded_y = encoder.transform(y)\n",
    "dummy_y = np_utils.to_categorical(encoded_y)\n",
    "\n",
    "pickle.dump(encoder, open('encoder.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, dummy_y, test_size=0.20, random_state=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform each example to a tf-idf vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf-idf with 5000 features\n"
     ]
    }
   ],
   "source": [
    "X_train_tfidf, X_test_tfidf = tfidf(X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a DNN model that takes 5,000 features as its input and outputs the probability vector of the four classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_DNN_model(X_train_tfidf.shape[1], 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the model summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 512)               2560512   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 4)                 2052      \n",
      "=================================================================\n",
      "Total params: 3,613,188\n",
      "Trainable params: 3,613,188\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model for 5 epochs using 20 percent of the training data as the validation set and a batch size of 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1024 samples, validate on 256 samples\n",
      "Epoch 1/5\n",
      "1024/1024 [==============================] - 1s 1ms/step - loss: 1.1117 - accuracy: 0.6123 - val_loss: 0.2994 - val_accuracy: 0.9414\n",
      "Epoch 2/5\n",
      "1024/1024 [==============================] - 1s 832us/step - loss: 0.1853 - accuracy: 0.9395 - val_loss: 0.1079 - val_accuracy: 0.9414\n",
      "Epoch 3/5\n",
      "1024/1024 [==============================] - 1s 867us/step - loss: 0.0819 - accuracy: 0.9658 - val_loss: 0.0727 - val_accuracy: 0.9844\n",
      "Epoch 4/5\n",
      "1024/1024 [==============================] - 1s 854us/step - loss: 0.0381 - accuracy: 0.9971 - val_loss: 0.0076 - val_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "1024/1024 [==============================] - 1s 853us/step - loss: 0.0040 - accuracy: 0.9990 - val_loss: 0.0273 - val_accuracy: 0.9961\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_tfidf, y_train, validation_split=0.2, epochs=5, batch_size=128, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Evaluate the trained model against the test dataset. With only 5 epochs, the model seems to be able to generalise reasonably well with the accuracy of 99.69 percent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320/320 [==============================] - 0s 371us/step\n",
      "Test Score: 0.006668820339473314\n",
      "Test Accuracy: 0.996874988079071\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test_tfidf, y_test, verbose=1)\n",
    "\n",
    "print(\"Test Score:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the trained model as a HDF5 file. For testing purposes, delete the existing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5')  # creates a HDF5 file 'model.h5'\n",
    "del model  # deletes the existing model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a better understanding of model performance over the whole dataset instead of just a single train/test split, cross validation should be performed to train and test the model over multiple folds of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def tfidf_all(X, num_words=5000):\n",
    "\n",
    "    vectorizer_x = TfidfVectorizer(max_features=num_words)\n",
    "    X = vectorizer_x.fit_transform(X).toarray()\n",
    "\n",
    "    print(\"tf-idf with\", str(np.array(X).shape[1]), \"features\")\n",
    "\n",
    "    return X\n",
    "\n",
    "def DNN_model():\n",
    "\n",
    "    model = Sequential()\n",
    "    node = 512 # number of nodes\n",
    "    nLayers = 4 # number of  hidden layer\n",
    "    model.add(Dense(node,input_dim=5000,activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    for i in range(0,nLayers):\n",
    "        model.add(Dense(node,input_dim=node,activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf-idf with 5000 features\n",
      "Epoch 1/5\n",
      "1280/1280 [==============================] - 1s 1ms/step - loss: 0.9338 - accuracy: 0.7273\n",
      "Epoch 2/5\n",
      "1280/1280 [==============================] - 1s 849us/step - loss: 0.1326 - accuracy: 0.9477\n",
      "Epoch 3/5\n",
      "1280/1280 [==============================] - 1s 893us/step - loss: 0.0355 - accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "1280/1280 [==============================] - 1s 1ms/step - loss: 8.0562e-04 - accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "1280/1280 [==============================] - 1s 958us/step - loss: 9.0529e-06 - accuracy: 1.0000\n",
      "320/320 [==============================] - 0s 315us/step\n",
      "Epoch 1/5\n",
      "1280/1280 [==============================] - 2s 1ms/step - loss: 0.9856 - accuracy: 0.6531\n",
      "Epoch 2/5\n",
      "1280/1280 [==============================] - 1s 861us/step - loss: 0.1275 - accuracy: 0.9477\n",
      "Epoch 3/5\n",
      "1280/1280 [==============================] - 1s 982us/step - loss: 0.0543 - accuracy: 0.9969\n",
      "Epoch 4/5\n",
      "1280/1280 [==============================] - 1s 769us/step - loss: 0.0060 - accuracy: 0.9984\n",
      "Epoch 5/5\n",
      "1280/1280 [==============================] - 1s 784us/step - loss: 4.2236e-04 - accuracy: 1.0000\n",
      "320/320 [==============================] - 0s 374us/step\n",
      "Epoch 1/5\n",
      "1280/1280 [==============================] - 1s 959us/step - loss: 0.9367 - accuracy: 0.6734\n",
      "Epoch 2/5\n",
      "1280/1280 [==============================] - 1s 758us/step - loss: 0.1351 - accuracy: 0.9484\n",
      "Epoch 3/5\n",
      "1280/1280 [==============================] - 1s 779us/step - loss: 0.0397 - accuracy: 0.9898\n",
      "Epoch 4/5\n",
      "1280/1280 [==============================] - 1s 955us/step - loss: 0.0043 - accuracy: 0.9984\n",
      "Epoch 5/5\n",
      "1280/1280 [==============================] - 1s 834us/step - loss: 3.5610e-04 - accuracy: 1.0000\n",
      "320/320 [==============================] - 0s 346us/step\n",
      "Epoch 1/5\n",
      "1280/1280 [==============================] - 1s 998us/step - loss: 0.9041 - accuracy: 0.7367\n",
      "Epoch 2/5\n",
      "1280/1280 [==============================] - 1s 806us/step - loss: 0.1142 - accuracy: 0.9594\n",
      "Epoch 3/5\n",
      "1280/1280 [==============================] - 1s 773us/step - loss: 0.0257 - accuracy: 0.9992\n",
      "Epoch 4/5\n",
      "1280/1280 [==============================] - 1s 798us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "1280/1280 [==============================] - 1s 789us/step - loss: 4.8144e-05 - accuracy: 1.0000\n",
      "320/320 [==============================] - 0s 333us/step\n",
      "Epoch 1/5\n",
      "1280/1280 [==============================] - 1s 1ms/step - loss: 0.9419 - accuracy: 0.7617\n",
      "Epoch 2/5\n",
      "1280/1280 [==============================] - 1s 1ms/step - loss: 0.1160 - accuracy: 0.9516\n",
      "Epoch 3/5\n",
      "1280/1280 [==============================] - 1s 998us/step - loss: 0.0343 - accuracy: 0.9969\n",
      "Epoch 4/5\n",
      "1280/1280 [==============================] - 1s 889us/step - loss: 0.0041 - accuracy: 0.9992\n",
      "Epoch 5/5\n",
      "1280/1280 [==============================] - 1s 784us/step - loss: 0.0044 - accuracy: 0.9977\n",
      "320/320 [==============================] - 0s 424us/step\n",
      "Accuracy: 99.62% (0.23%)\n"
     ]
    }
   ],
   "source": [
    "estimator = KerasClassifier(build_fn=DNN_model, epochs=5, batch_size=128, verbose=1)\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "results = cross_val_score(estimator, tfidf_all(X), dummy_y, cv=kfold)\n",
    "\n",
    "print(\"Accuracy: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Saved Model\n",
    "\n",
    "Load the saved trained model along with the input vectoriser and label encoder objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('model.h5')\n",
    "vectoriser = pickle.load(open('vectoriser.pkl', 'rb'))\n",
    "encoder = pickle.load(open('encoder.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessing process should remain the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    processed = re.sub('[^a-zA-Z]', ' ', text)\n",
    "    processed = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', processed)\n",
    "    processed = re.sub(r'\\s+', ' ', processed)\n",
    "\n",
    "    return processed\n",
    "\n",
    "def process_input_data(paths):\n",
    "    data = []\n",
    "    \n",
    "    for path in paths:\n",
    "        print('Processing {}'.format(path))\n",
    "        parsed = tika_parser.from_file(path)\n",
    "        text = preprocess_text(parsed[\"content\"])\n",
    "        data.append(text)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select a document from one of the files and then preprocess the data using the alredy defined function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing C:\\Users\\Alex.Chokwijitkul\\Desktop\\Document Classification\\data\\Papers\\1812.02993.pdf\n"
     ]
    }
   ],
   "source": [
    "papers = process_input_data([paper_paths[10]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the data to a feature vector using the loaded vectoriser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector = vectoriser.transform([papers[0]]).toarray()\n",
    "vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the loaded model to predict the input vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.7061787e-13, 1.8738636e-22, 1.0000000e+00, 1.0185268e-28]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = model.predict([vector])\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Round up the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 0.], dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = np.round(prediction[0])\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Translate the result back to a string label. The output is \"paper\", which is a correct label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'paper'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = encoder.inverse_transform(np.where(prediction == 1))\n",
    "prediction[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
