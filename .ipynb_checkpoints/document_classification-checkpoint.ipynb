{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import  Dropout, Dense\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "from keras.models import load_model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn import metrics\n",
    "import pickle\n",
    "import tika\n",
    "import glob\n",
    "from tika import parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"C:\\\\Users\\\\Alex.Chokwijitkul\\\\Desktop\\\\Document Classification\\\\data\\\\\"\n",
    "\n",
    "agenda_paths = [f for f in glob.glob(root_path + \"Agendas/*\", recursive=False)]\n",
    "medicalrecord_paths = [f for f in glob.glob(root_path + \"MedicalRecords/*\", recursive=False)]\n",
    "paper_paths = [f for f in glob.glob(root_path + \"Papers/*\", recursive=False)]\n",
    "resume_paths = [f for f in glob.glob(root_path + \"Resumes/*\", recursive=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    processed = re.sub('[^a-zA-Z]', ' ', text)\n",
    "    processed = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', processed)\n",
    "    processed = re.sub(r'\\s+', ' ', processed)\n",
    "\n",
    "    return processed\n",
    "\n",
    "def process_raw_data(paths, label):\n",
    "    data = {\n",
    "        'Content': [],\n",
    "        'Type': [label] * len(paths)\n",
    "    }\n",
    "    \n",
    "    for path in paths:\n",
    "        print('Processing {}'.format(path))\n",
    "        parsed = parser.from_file(path)\n",
    "        text = preprocess_text(parsed[\"content\"])\n",
    "        data['Content'].append(text)\n",
    "    \n",
    "    return pd.DataFrame(data, columns = ['Content', 'Type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agenda_df = process_raw_data(agenda_paths, 'agenda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medicalrecord_df = process_raw_data(medicalrecord_paths, 'medicalrecord')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_df = process_raw_data(paper_paths, 'paper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_df = process_raw_data(resume_paths, 'resume')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([agenda_df, medicalrecord_df, paper_df, resume_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(X_train, X_test, num_words=5000):\n",
    "\n",
    "    vectorizer_x = TfidfVectorizer(max_features=num_words)\n",
    "    X_train = vectorizer_x.fit_transform(X_train).toarray()\n",
    "    X_test = vectorizer_x.transform(X_test).toarray()\n",
    "\n",
    "    pickle.dump(vectorizer_x, open('vectoriser.pkl','wb'))\n",
    "    print(\"tf-idf with\", str(np.array(X_train).shape[1]), \"features\")\n",
    "\n",
    "    return (X_train,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_DNN_model(shape, num_classes, dropout=0.2):\n",
    "\n",
    "    model = Sequential()\n",
    "    node = 512 # number of nodes\n",
    "    num_layers = 4 # number of  hidden layer\n",
    "    model.add(Dense(node,input_dim=shape,activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    for i in range(0, num_layers):\n",
    "        model.add(Dense(node,input_dim=node,activation='relu'))\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['Content'].values\n",
    "y = df['Type'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "encoded_y = encoder.transform(y)\n",
    "dummy_y = np_utils.to_categorical(encoded_y)\n",
    "\n",
    "pickle.dump(encoder, open('encoder.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, dummy_y, test_size=0.20, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf-idf with 5000 features\n"
     ]
    }
   ],
   "source": [
    "X_train_tfidf, X_test_tfidf = tfidf(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_DNN_model(X_train_tfidf.shape[1], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 512)               2560512   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 4)                 2052      \n",
      "=================================================================\n",
      "Total params: 3,613,188\n",
      "Trainable params: 3,613,188\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1024 samples, validate on 256 samples\n",
      "Epoch 1/5\n",
      "1024/1024 [==============================] - 1s 1ms/step - loss: 1.1117 - accuracy: 0.6123 - val_loss: 0.2994 - val_accuracy: 0.9414\n",
      "Epoch 2/5\n",
      "1024/1024 [==============================] - 1s 832us/step - loss: 0.1853 - accuracy: 0.9395 - val_loss: 0.1079 - val_accuracy: 0.9414\n",
      "Epoch 3/5\n",
      "1024/1024 [==============================] - 1s 867us/step - loss: 0.0819 - accuracy: 0.9658 - val_loss: 0.0727 - val_accuracy: 0.9844\n",
      "Epoch 4/5\n",
      "1024/1024 [==============================] - 1s 854us/step - loss: 0.0381 - accuracy: 0.9971 - val_loss: 0.0076 - val_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "1024/1024 [==============================] - 1s 853us/step - loss: 0.0040 - accuracy: 0.9990 - val_loss: 0.0273 - val_accuracy: 0.9961\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_tfidf, y_train, validation_split=0.2, epochs=5, batch_size=128, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320/320 [==============================] - 0s 371us/step\n",
      "Test Score: 0.006668820339473314\n",
      "Test Accuracy: 0.996874988079071\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test_tfidf, y_test, verbose=1)\n",
    "\n",
    "print(\"Test Score:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5')  # creates a HDF5 file 'model.h5'\n",
    "del model  # deletes the existing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def tfidf_all(X, num_words=5000):\n",
    "\n",
    "    vectorizer_x = TfidfVectorizer(max_features=num_words)\n",
    "    X = vectorizer_x.fit_transform(X).toarray()\n",
    "\n",
    "    print(\"tf-idf with\", str(np.array(X).shape[1]), \"features\")\n",
    "\n",
    "    return X\n",
    "\n",
    "def DNN_model():\n",
    "\n",
    "    model = Sequential()\n",
    "    node = 512 # number of nodes\n",
    "    nLayers = 4 # number of  hidden layer\n",
    "    model.add(Dense(node,input_dim=5000,activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    for i in range(0,nLayers):\n",
    "        model.add(Dense(node,input_dim=node,activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf-idf with 5000 features\n",
      "Epoch 1/5\n",
      "1280/1280 [==============================] - 1s 1ms/step - loss: 0.9338 - accuracy: 0.7273\n",
      "Epoch 2/5\n",
      "1280/1280 [==============================] - 1s 849us/step - loss: 0.1326 - accuracy: 0.9477\n",
      "Epoch 3/5\n",
      "1280/1280 [==============================] - 1s 893us/step - loss: 0.0355 - accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "1280/1280 [==============================] - 1s 1ms/step - loss: 8.0562e-04 - accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "1280/1280 [==============================] - 1s 958us/step - loss: 9.0529e-06 - accuracy: 1.0000\n",
      "320/320 [==============================] - 0s 315us/step\n",
      "Epoch 1/5\n",
      "1280/1280 [==============================] - 2s 1ms/step - loss: 0.9856 - accuracy: 0.6531\n",
      "Epoch 2/5\n",
      "1280/1280 [==============================] - 1s 861us/step - loss: 0.1275 - accuracy: 0.9477\n",
      "Epoch 3/5\n",
      "1280/1280 [==============================] - 1s 982us/step - loss: 0.0543 - accuracy: 0.9969\n",
      "Epoch 4/5\n",
      "1280/1280 [==============================] - 1s 769us/step - loss: 0.0060 - accuracy: 0.9984\n",
      "Epoch 5/5\n",
      "1280/1280 [==============================] - 1s 784us/step - loss: 4.2236e-04 - accuracy: 1.0000\n",
      "320/320 [==============================] - 0s 374us/step\n",
      "Epoch 1/5\n",
      "1280/1280 [==============================] - 1s 959us/step - loss: 0.9367 - accuracy: 0.6734\n",
      "Epoch 2/5\n",
      "1280/1280 [==============================] - 1s 758us/step - loss: 0.1351 - accuracy: 0.9484\n",
      "Epoch 3/5\n",
      "1280/1280 [==============================] - 1s 779us/step - loss: 0.0397 - accuracy: 0.9898\n",
      "Epoch 4/5\n",
      "1280/1280 [==============================] - 1s 955us/step - loss: 0.0043 - accuracy: 0.9984\n",
      "Epoch 5/5\n",
      "1280/1280 [==============================] - 1s 834us/step - loss: 3.5610e-04 - accuracy: 1.0000\n",
      "320/320 [==============================] - 0s 346us/step\n",
      "Epoch 1/5\n",
      "1280/1280 [==============================] - 1s 998us/step - loss: 0.9041 - accuracy: 0.7367\n",
      "Epoch 2/5\n",
      "1280/1280 [==============================] - 1s 806us/step - loss: 0.1142 - accuracy: 0.9594\n",
      "Epoch 3/5\n",
      "1280/1280 [==============================] - 1s 773us/step - loss: 0.0257 - accuracy: 0.9992\n",
      "Epoch 4/5\n",
      "1280/1280 [==============================] - 1s 798us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "1280/1280 [==============================] - 1s 789us/step - loss: 4.8144e-05 - accuracy: 1.0000\n",
      "320/320 [==============================] - 0s 333us/step\n",
      "Epoch 1/5\n",
      "1280/1280 [==============================] - 1s 1ms/step - loss: 0.9419 - accuracy: 0.7617\n",
      "Epoch 2/5\n",
      "1280/1280 [==============================] - 1s 1ms/step - loss: 0.1160 - accuracy: 0.9516\n",
      "Epoch 3/5\n",
      "1280/1280 [==============================] - 1s 998us/step - loss: 0.0343 - accuracy: 0.9969\n",
      "Epoch 4/5\n",
      "1280/1280 [==============================] - 1s 889us/step - loss: 0.0041 - accuracy: 0.9992\n",
      "Epoch 5/5\n",
      "1280/1280 [==============================] - 1s 784us/step - loss: 0.0044 - accuracy: 0.9977\n",
      "320/320 [==============================] - 0s 424us/step\n",
      "Accuracy: 99.62% (0.23%)\n"
     ]
    }
   ],
   "source": [
    "estimator = KerasClassifier(build_fn=DNN_model, epochs=5, batch_size=128, verbose=1)\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "results = cross_val_score(estimator, tfidf_all(X), dummy_y, cv=kfold)\n",
    "\n",
    "print(\"Accuracy: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('model.h5')\n",
    "vectoriser = pickle.load(open('vectoriser.pkl', 'rb'))\n",
    "encoder = pickle.load(open('encoder.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    processed = re.sub('[^a-zA-Z]', ' ', text)\n",
    "    processed = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', processed)\n",
    "    processed = re.sub(r'\\s+', ' ', processed)\n",
    "\n",
    "    return processed\n",
    "\n",
    "def process_input_data(paths):\n",
    "    data = []\n",
    "    \n",
    "    for path in paths:\n",
    "        print('Processing {}'.format(path))\n",
    "        parsed = parser.from_file(path)\n",
    "        text = preprocess_text(parsed[\"content\"])\n",
    "        data.append(text)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing C:\\Users\\Alex.Chokwijitkul\\Desktop\\Document Classification\\data\\Papers\\1812.02993.pdf\n"
     ]
    }
   ],
   "source": [
    "papers = process_input_data([paper_paths[10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector = vectoriser.transform([papers[0]]).toarray()\n",
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.7061787e-13, 1.8738636e-22, 1.0000000e+00, 1.0185268e-28]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = model.predict([vector])\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 0.], dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = np.round(prediction[0])\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'paper'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = encoder.inverse_transform(np.where(prediction == 1))\n",
    "prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
