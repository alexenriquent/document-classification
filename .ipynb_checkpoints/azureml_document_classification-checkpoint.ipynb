{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to use Azure ML 1.0.72 to work with MachineLearning\n"
     ]
    }
   ],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.get(name=\"MachineLearning\",\n",
    "               subscription_id='<subscription_id>',\n",
    "               resource_group='MachineLearning')\n",
    "\n",
    "print('Ready to use Azure ML {} to work with {}'.format(azureml.core.VERSION, ws.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datastore already registered.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from azureml.core import Datastore\n",
    "\n",
    "if 'mldatastore'not in ws.datastores:\n",
    "    blob_datastore_name='mldatastore' # Name of the datastore to workspace\n",
    "    container_name=os.getenv(\"BLOB_CONTAINER\", \"mldata\") # Name of Azure blob container\n",
    "    account_name=os.getenv(\"BLOB_ACCOUNTNAME\", \"alexmlstorage\") # Storage account name\n",
    "    account_key=os.getenv(\"BLOB_ACCOUNT_KEY\", \"<BLOB_ACCOUNT_KEY>\") # Storage account key\n",
    "    \n",
    "    try:\n",
    "        blob_ds = Datastore.register_azure_blob_container(workspace=ws, \n",
    "                                                                 datastore_name=blob_datastore_name, \n",
    "                                                                 container_name=container_name, \n",
    "                                                                 account_name=account_name,\n",
    "                                                                 account_key=account_key)\n",
    "        print('Datastore registered.')\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "else:\n",
    "    blob_ds = Datastore.get(ws, datastore_name='mldatastore')\n",
    "    print('Datastore already registered.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already registered.\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Dataset\n",
    "\n",
    "if 'document_files' not in ws.datasets:\n",
    "    #Create a file dataset from the path on the datastore.\n",
    "    document_ds = Dataset.File.from_files(path=(blob_ds, ''))\n",
    "\n",
    "    # Register the file dataset\n",
    "    try:\n",
    "        document_ds = document_ds.register(workspace=ws, \n",
    "                                            name='document_files',\n",
    "                                            description='documents',\n",
    "                                            create_new_version=True)\n",
    "        print('Dataset registered.')\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "else:\n",
    "    document_ds = Dataset.get_by_name(ws, 'document_files')\n",
    "    print('Dataset already registered.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing cluster, use it.\n",
      "Succeeded\n",
      "AmlCompute wait for completion finished\n",
      "Minimum number of nodes requested have been provisioned\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "cluster_name = 'aml-cluster'\n",
    "\n",
    "try:\n",
    "    # Check for existing compute target\n",
    "    training_cluster = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    # If it doesn't already exist, create it\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS12_V2', max_nodes=4)\n",
    "    training_cluster = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "\n",
    "training_cluster.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment folder created\n"
     ]
    }
   ],
   "source": [
    "experiment_folder = 'experiment'\n",
    "os.makedirs(experiment_folder, exist_ok=True)\n",
    "\n",
    "print(experiment_folder, 'folder created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting experiment/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $experiment_folder/train.py\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "import tika\n",
    "import glob\n",
    "from keras.layers import  Dropout, Dense\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "from keras.models import load_model\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tika import parser as tika_parser\n",
    "from azureml.core import Run\n",
    "from azureml.core import Experiment\n",
    "from azureml.core import Model\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data-folder', type=str, dest='data_folder', help='data folder mounting point')\n",
    "args = parser.parse_args()\n",
    "\n",
    "data_folder = args.data_folder\n",
    "print('Data folder:', data_folder)\n",
    "\n",
    "run = Run.get_context()\n",
    "\n",
    "print('Loading Data...')\n",
    "agenda_paths = glob.glob(data_folder + \"**/Agendas/*\")\n",
    "medicalrecord_paths = glob.glob(data_folder + \"**/MedicalRecords/*\")\n",
    "paper_paths = glob.glob(data_folder + \"**/Papers/*\")\n",
    "resume_paths = glob.glob(data_folder + \"**/Resumes/*\")\n",
    "\n",
    "def preprocess_text(text):\n",
    "    processed = re.sub('[^a-zA-Z]', ' ', text)\n",
    "    processed = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', processed)\n",
    "    processed = re.sub(r'\\s+', ' ', processed)\n",
    "\n",
    "    return processed\n",
    "\n",
    "def process_raw_data(paths, label):\n",
    "    data = {\n",
    "        'Content': [],\n",
    "        'Type': [label] * len(paths)\n",
    "    }\n",
    "    \n",
    "    for path in paths:\n",
    "        print('Processing {}'.format(path))\n",
    "        parsed = tika_parser.from_file(path)\n",
    "        text = preprocess_text(parsed[\"content\"])\n",
    "        data['Content'].append(text)\n",
    "    \n",
    "    return pd.DataFrame(data, columns = ['Content', 'Type'])\n",
    "\n",
    "def tfidf(X_train, X_test, num_words=5000):\n",
    "\n",
    "    vectorizer_x = TfidfVectorizer(max_features=num_words)\n",
    "    X_train = vectorizer_x.fit_transform(X_train).toarray()\n",
    "    X_test = vectorizer_x.transform(X_test).toarray()\n",
    "\n",
    "    vectoriser_file = 'vectoriser.pkl'\n",
    "    pickle.dump(vectorizer_x, open(vectoriser_file,'wb'))\n",
    "    run.upload_file(name = 'models/' + vectoriser_file, path_or_stream = './' + vectoriser_file)\n",
    "    \n",
    "    print(\"tf-idf with\", str(np.array(X_train).shape[1]), \"features\")\n",
    "\n",
    "    return (X_train,X_test)\n",
    "\n",
    "def build_DNN_model(shape, num_classes, dropout=0.2):\n",
    "\n",
    "    model = Sequential()\n",
    "    node = 512 # number of nodes\n",
    "    num_layers = 4 # number of  hidden layer\n",
    "    model.add(Dense(node,input_dim=shape,activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    for i in range(0, num_layers):\n",
    "        model.add(Dense(node,input_dim=node,activation='relu'))\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "agenda_df = process_raw_data(agenda_paths, 'agenda')\n",
    "medicalrecord_df = process_raw_data(medicalrecord_paths, 'medicalrecord')\n",
    "paper_df = process_raw_data(paper_paths, 'paper')\n",
    "resume_df = process_raw_data(resume_paths, 'resume')\n",
    "\n",
    "df = pd.concat([agenda_df, medicalrecord_df, paper_df, resume_df], axis=0)\n",
    "\n",
    "X = df['Content'].values\n",
    "y = df['Type'].values\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "encoded_y = encoder.transform(y)\n",
    "dummy_y = np_utils.to_categorical(encoded_y)\n",
    "\n",
    "encoder_file = 'encoder.pkl'\n",
    "pickle.dump(encoder, open(encoder_file,'wb'))\n",
    "run.upload_file(name = 'models/' + encoder_file, path_or_stream = './' + encoder_file)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, dummy_y, test_size=0.20, random_state=7)\n",
    "X_train_tfidf, X_test_tfidf = tfidf(X_train, X_test)\n",
    "\n",
    "model = build_DNN_model(X_train_tfidf.shape[1], 4)\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "history = model.fit(X_train_tfidf, y_train, validation_split=0.2, epochs=5, batch_size=128, verbose=1)\n",
    "score = model.evaluate(X_test_tfidf, y_test, verbose=1)\n",
    "\n",
    "print(\"Test Score:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n",
    "\n",
    "run.log('Score', np.float(score[0]))\n",
    "run.log('Accuracy', np.float(score[1]))\n",
    "\n",
    "dnn_file = 'dnn.h5'\n",
    "model.save(dnn_file) \n",
    "run.upload_file(name = 'models/' + dnn_file, path_or_stream = './' + dnn_file)\n",
    "\n",
    "run.complete()\n",
    "\n",
    "run.register_model(model_path='./models', \n",
    "                   model_name='document_classification_model',\n",
    "                   description='Document classification model',\n",
    "                   properties={'Accuracy': run.get_metrics()['Accuracy']})\n",
    "\n",
    "print('Model trained and registered.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment registered.\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.environment import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "env = Environment('document-classification')\n",
    "\n",
    "env.docker.enabled = True\n",
    "env.docker.base_image = \"intelmpi:v1\"\n",
    "env.docker.base_image_registry.address = \"machinelearndcc81c93.azurecr.io\"\n",
    "env.docker.base_image_registry.username = \"machinelearndcc81c93\"\n",
    "env.docker.base_image_registry.password = \"<container_registry_password>\"\n",
    "\n",
    "# to install required packages\n",
    "conda_packages = ['numpy', 'scikit-learn']\n",
    "pip_packages = ['azureml-dataprep[pandas,fuse]', 'azureml-defaults', 'tensorflow', 'keras', 'tika']\n",
    "\n",
    "cd = CondaDependencies.create(pip_packages=pip_packages, conda_packages=conda_packages)\n",
    "env.python.conda_dependencies = cd\n",
    "    \n",
    "# Register environment to re-use later\n",
    "env.register(workspace = ws)\n",
    "print('Environment registered.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fca51f2e4b0b4750955c6edb8ccedceb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', '…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/aml.mini.widget.v1": "{\"status\": \"Running\", \"workbench_run_details_uri\": \"https://ml.azure.com/experiments/documentclassification-training/runs/documentclassification-training_1586586326_a07bcf03?wsid=/subscriptions/41e7097b-b33b-4dad-ba38-6a953141427e/resourcegroups/MachineLearning/workspaces/MachineLearning\", \"run_id\": \"documentclassification-training_1586586326_a07bcf03\", \"run_properties\": {\"run_id\": \"documentclassification-training_1586586326_a07bcf03\", \"created_utc\": \"2020-04-11T06:25:38.214289Z\", \"properties\": {\"_azureml.ComputeTargetType\": \"amlcompute\", \"ContentSnapshotId\": \"1b8ba5d4-b1c0-49eb-8ff0-832bccc973ce\", \"AzureML.DerivedImageName\": \"azureml/azureml_13204f7f39d612b4bfd4cf1d9fa89800\", \"ProcessInfoFile\": \"azureml-logs/process_info.json\", \"ProcessStatusFile\": \"azureml-logs/process_status.json\"}, \"tags\": {\"_aml_system_ComputeTargetStatus\": \"{\\\"AllocationState\\\":\\\"steady\\\",\\\"PreparingNodeCount\\\":0,\\\"RunningNodeCount\\\":0,\\\"CurrentNodeCount\\\":0}\"}, \"script_name\": null, \"arguments\": null, \"end_time_utc\": null, \"status\": \"Running\", \"log_files\": {\"azureml-logs/55_azureml-execution-tvmps_0acf047066a72a9022000013c117b570088bfc93b9bc350f33948147bd43a5ae_d.txt\": \"https://machinelearnin7708334383.blob.core.windows.net/azureml/ExperimentRun/dcid.documentclassification-training_1586586326_a07bcf03/azureml-logs/55_azureml-execution-tvmps_0acf047066a72a9022000013c117b570088bfc93b9bc350f33948147bd43a5ae_d.txt?sv=2019-02-02&sr=b&sig=5cg2xzniovhfuhchlACPQg4Ev3mTGRodf95h9S9B0AI%3D&st=2020-04-11T06%3A20%3A44Z&se=2020-04-11T14%3A30%3A44Z&sp=r\"}, \"log_groups\": [[\"azureml-logs/55_azureml-execution-tvmps_0acf047066a72a9022000013c117b570088bfc93b9bc350f33948147bd43a5ae_d.txt\"]], \"run_duration\": \"0:05:03\"}, \"child_runs\": [], \"children_metrics\": {}, \"run_metrics\": [], \"run_logs\": \"2020-04-11T06:29:52Z Starting output-watcher...\\n2020-04-11T06:29:52Z IsDedicatedCompute == True, won't poll for Low Pri Preemption\\nLogin Succeeded\\nUsing default tag: latest\\nlatest: Pulling from azureml/azureml_13204f7f39d612b4bfd4cf1d9fa89800\\nfe703b657a32: Pulling fs layer\\nf9df1fafd224: Pulling fs layer\\na645a4b887f9: Pulling fs layer\\n57db7fe0b522: Pulling fs layer\\n489c184a3ffd: Pulling fs layer\\n2f1d0788913d: Pulling fs layer\\n9adaf8fba06e: Pulling fs layer\\n32c8ffcdd900: Pulling fs layer\\n4ba50e238086: Pulling fs layer\\n1ad9a6b50574: Pulling fs layer\\n11193a8b5b81: Pulling fs layer\\n13e8cf02786b: Pulling fs layer\\n744853fbb531: Pulling fs layer\\n5603221065b6: Pulling fs layer\\nf615701f82d4: Pulling fs layer\\n32c8ffcdd900: Waiting\\n4ba50e238086: Waiting\\n489c184a3ffd: Waiting\\n1ad9a6b50574: Waiting\\n11193a8b5b81: Waiting\\n2f1d0788913d: Waiting\\n5603221065b6: Waiting\\n13e8cf02786b: Waiting\\nf615701f82d4: Waiting\\n744853fbb531: Waiting\\n9adaf8fba06e: Waiting\\n57db7fe0b522: Waiting\\nf9df1fafd224: Verifying Checksum\\nf9df1fafd224: Download complete\\n57db7fe0b522: Verifying Checksum\\n57db7fe0b522: Download complete\\nfe703b657a32: Verifying Checksum\\nfe703b657a32: Download complete\\n489c184a3ffd: Verifying Checksum\\n489c184a3ffd: Download complete\\n2f1d0788913d: Verifying Checksum\\n2f1d0788913d: Download complete\\n4ba50e238086: Download complete\\n1ad9a6b50574: Verifying Checksum\\n1ad9a6b50574: Download complete\\n9adaf8fba06e: Verifying Checksum\\n9adaf8fba06e: Download complete\\n11193a8b5b81: Verifying Checksum\\n11193a8b5b81: Download complete\\n13e8cf02786b: Verifying Checksum\\n13e8cf02786b: Download complete\\n744853fbb531: Download complete\\nf615701f82d4: Verifying Checksum\\n32c8ffcdd900: Verifying Checksum\\n32c8ffcdd900: Download complete\\nfe703b657a32: Pull complete\\nf9df1fafd224: Pull complete\\na645a4b887f9: Pull complete\\n57db7fe0b522: Pull complete\\n489c184a3ffd: Pull complete\\n5603221065b6: Verifying Checksum\\n5603221065b6: Download complete\\n2f1d0788913d: Pull complete\\n9adaf8fba06e: Pull complete\\n32c8ffcdd900: Pull complete\\n4ba50e238086: Pull complete\\n1ad9a6b50574: Pull complete\\n11193a8b5b81: Pull complete\\n13e8cf02786b: Pull complete\\n744853fbb531: Pull complete\\n\", \"graph\": {}, \"widget_settings\": {\"childWidgetDisplay\": \"popup\", \"send_telemetry\": false, \"log_level\": \"INFO\", \"sdk_version\": \"1.0.72\"}, \"loading\": false}"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'runId': 'documentclassification-training_1586586326_a07bcf03',\n",
       " 'target': 'aml-cluster',\n",
       " 'status': 'Finalizing',\n",
       " 'startTimeUtc': '2020-04-11T06:29:48.994886Z',\n",
       " 'properties': {'_azureml.ComputeTargetType': 'amlcompute',\n",
       "  'ContentSnapshotId': '1b8ba5d4-b1c0-49eb-8ff0-832bccc973ce',\n",
       "  'AzureML.DerivedImageName': 'azureml/azureml_13204f7f39d612b4bfd4cf1d9fa89800',\n",
       "  'ProcessInfoFile': 'azureml-logs/process_info.json',\n",
       "  'ProcessStatusFile': 'azureml-logs/process_status.json'},\n",
       " 'inputDatasets': [{'dataset': {'id': '0d2fdb57-6235-4f6a-9c07-32593b9df03d'}, 'consumptionDetails': {'type': 'RunInput', 'inputName': 'documents', 'mechanism': 'Mount'}}],\n",
       " 'runDefinition': {'script': 'train.py',\n",
       "  'useAbsolutePath': False,\n",
       "  'arguments': ['--data-folder', 'DatasetConsumptionConfig:documents'],\n",
       "  'sourceDirectoryDataStore': None,\n",
       "  'framework': 'Python',\n",
       "  'communicator': 'None',\n",
       "  'target': 'aml-cluster',\n",
       "  'dataReferences': {},\n",
       "  'data': {'documents': {'dataLocation': {'dataset': {'id': '0d2fdb57-6235-4f6a-9c07-32593b9df03d'},\n",
       "     'dataPath': None},\n",
       "    'createOutputDirectories': False,\n",
       "    'mechanism': 'Mount',\n",
       "    'environmentVariableName': 'documents',\n",
       "    'pathOnCompute': None,\n",
       "    'overwrite': False}},\n",
       "  'jobName': None,\n",
       "  'maxRunDurationSeconds': None,\n",
       "  'nodeCount': 1,\n",
       "  'environment': {'name': 'document-classification',\n",
       "   'version': '10',\n",
       "   'python': {'interpreterPath': 'python',\n",
       "    'userManagedDependencies': False,\n",
       "    'condaDependencies': {'channels': ['conda-forge'],\n",
       "     'dependencies': ['python=3.6.2',\n",
       "      {'pip': ['azureml-dataprep[pandas,fuse]',\n",
       "        'azureml-defaults==1.0.72.*',\n",
       "        'tensorflow',\n",
       "        'keras',\n",
       "        'tika']},\n",
       "      'numpy',\n",
       "      'scikit-learn'],\n",
       "     'name': 'azureml_912ca7bbac4f0ab7a103e2c81943a708'},\n",
       "    'baseCondaEnvironment': None},\n",
       "   'environmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE'},\n",
       "   'docker': {'baseImage': 'intelmpi:v1',\n",
       "    'baseDockerfile': None,\n",
       "    'baseImageRegistry': {'address': 'machinelearndcc81c93.azurecr.io',\n",
       "     'username': 'machinelearndcc81c93',\n",
       "     'password': 'AzureMlSecret=documentclassification-training_1586586326_a07bcf03#RunConfiguration#ContainerRegistry#Password'},\n",
       "    'enabled': True,\n",
       "    'arguments': []},\n",
       "   'spark': {'repositories': [], 'packages': [], 'precachePackages': True},\n",
       "   'inferencingStackVersion': None},\n",
       "  'history': {'outputCollection': True,\n",
       "   'directoriesToWatch': ['logs'],\n",
       "   'snapshotProject': True},\n",
       "  'spark': {'configuration': {'spark.app.name': 'Azure ML Experiment',\n",
       "    'spark.yarn.maxAppAttempts': '1'}},\n",
       "  'amlCompute': {'name': None,\n",
       "   'vmSize': None,\n",
       "   'retainCluster': False,\n",
       "   'clusterMaxNodeCount': 1},\n",
       "  'tensorflow': {'workerCount': 1, 'parameterServerCount': 1},\n",
       "  'mpi': {'processCountPerNode': 1},\n",
       "  'hdi': {'yarnDeployMode': 'Cluster'},\n",
       "  'containerInstance': {'region': None, 'cpuCores': 2, 'memoryGb': 3.5},\n",
       "  'exposedPorts': None,\n",
       "  'docker': {'useDocker': True,\n",
       "   'sharedVolumes': True,\n",
       "   'shmSize': '2g',\n",
       "   'arguments': []},\n",
       "  'cmk8sCompute': {'configuration': {}}},\n",
       " 'logFiles': {'azureml-logs/55_azureml-execution-tvmps_0acf047066a72a9022000013c117b570088bfc93b9bc350f33948147bd43a5ae_d.txt': 'https://machinelearnin7708334383.blob.core.windows.net/azureml/ExperimentRun/dcid.documentclassification-training_1586586326_a07bcf03/azureml-logs/55_azureml-execution-tvmps_0acf047066a72a9022000013c117b570088bfc93b9bc350f33948147bd43a5ae_d.txt?sv=2019-02-02&sr=b&sig=BXlfFV%2Fc0of1d7xb3UuH4srqFlF6bvB3kM2qlD3DVrs%3D&st=2020-04-11T06%3A25%3A56Z&se=2020-04-11T14%3A35%3A56Z&sp=r',\n",
       "  'azureml-logs/65_job_prep-tvmps_0acf047066a72a9022000013c117b570088bfc93b9bc350f33948147bd43a5ae_d.txt': 'https://machinelearnin7708334383.blob.core.windows.net/azureml/ExperimentRun/dcid.documentclassification-training_1586586326_a07bcf03/azureml-logs/65_job_prep-tvmps_0acf047066a72a9022000013c117b570088bfc93b9bc350f33948147bd43a5ae_d.txt?sv=2019-02-02&sr=b&sig=1%2FKzw4fDUW9E6%2FqdRhiD5sOVIVkM4i891ghMlJDWIAM%3D&st=2020-04-11T06%3A25%3A56Z&se=2020-04-11T14%3A35%3A56Z&sp=r',\n",
       "  'azureml-logs/70_driver_log.txt': 'https://machinelearnin7708334383.blob.core.windows.net/azureml/ExperimentRun/dcid.documentclassification-training_1586586326_a07bcf03/azureml-logs/70_driver_log.txt?sv=2019-02-02&sr=b&sig=g3CDPY8%2Fq%2BB83qWxSYjhvnUTqj%2F1PN6gqgMr8R2YTk8%3D&st=2020-04-11T06%3A25%3A56Z&se=2020-04-11T14%3A35%3A56Z&sp=r',\n",
       "  'azureml-logs/75_job_post-tvmps_0acf047066a72a9022000013c117b570088bfc93b9bc350f33948147bd43a5ae_d.txt': 'https://machinelearnin7708334383.blob.core.windows.net/azureml/ExperimentRun/dcid.documentclassification-training_1586586326_a07bcf03/azureml-logs/75_job_post-tvmps_0acf047066a72a9022000013c117b570088bfc93b9bc350f33948147bd43a5ae_d.txt?sv=2019-02-02&sr=b&sig=4mtYlKYzDSk7ckMuwq7zeMOBk5u2J%2FdD0ocpgEph8Jw%3D&st=2020-04-11T06%3A25%3A56Z&se=2020-04-11T14%3A35%3A56Z&sp=r',\n",
       "  'azureml-logs/process_info.json': 'https://machinelearnin7708334383.blob.core.windows.net/azureml/ExperimentRun/dcid.documentclassification-training_1586586326_a07bcf03/azureml-logs/process_info.json?sv=2019-02-02&sr=b&sig=WpwKfElMoQoBqAXQWsZkBAIyMjEe1zHupxWsT0OkKYw%3D&st=2020-04-11T06%3A25%3A56Z&se=2020-04-11T14%3A35%3A56Z&sp=r',\n",
       "  'azureml-logs/process_status.json': 'https://machinelearnin7708334383.blob.core.windows.net/azureml/ExperimentRun/dcid.documentclassification-training_1586586326_a07bcf03/azureml-logs/process_status.json?sv=2019-02-02&sr=b&sig=C5T52tCG6gXVDqR2%2BONQbDb7R8Y4vR3VrK1DAYmz9y0%3D&st=2020-04-11T06%3A25%3A56Z&se=2020-04-11T14%3A35%3A56Z&sp=r'}}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.train.estimator import Estimator\n",
    "from azureml.core import Experiment\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "# Set the script parameters\n",
    "script_params = {\n",
    "    '--data-folder': document_ds.as_named_input('documents').as_mount()\n",
    "}\n",
    "\n",
    "# Get the training dataset\n",
    "diabetes_ds = ws.datasets.get(\"document_files\")\n",
    "\n",
    "# Create an estimator\n",
    "estimator = Estimator(source_directory=experiment_folder,\n",
    "                       entry_script='train.py',\n",
    "                       script_params=script_params,\n",
    "                       compute_target=cluster_name,\n",
    "                       environment_definition=env\n",
    "                      )\n",
    "\n",
    "# Create an experiment\n",
    "experiment_name = 'documentclassification-training'\n",
    "experiment = Experiment(workspace = ws, name = experiment_name)\n",
    "\n",
    "# Run the experiment\n",
    "run = experiment.submit(config=estimator)\n",
    "\n",
    "# Show the run details while running\n",
    "RunDetails(run).show()\n",
    "run.wait_for_completion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "service folder created.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "service_folder = 'service'\n",
    "\n",
    "# Create a folder for the web service files\n",
    "experiment_folder = './' + service_folder\n",
    "os.makedirs(service_folder, exist_ok=True)\n",
    "\n",
    "print(service_folder, 'folder created.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting service/score.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $service_folder/score.py\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import load_model\n",
    "from azureml.core.model import Model\n",
    "\n",
    "# Called when the service is loaded\n",
    "def init():\n",
    "    global model\n",
    "    global vectoriser\n",
    "    global encoder\n",
    "    \n",
    "    # Get the path where the deployed model can be found.\n",
    "    model_path = Model.get_model_path('document_classification_model')\n",
    "    \n",
    "    # load models\n",
    "    model = load_model(model_path + '/dnn.h5')\n",
    "\n",
    "    with open(model_path + '/vectoriser.pkl','rb') as handle:\n",
    "        vectoriser = pickle.load(handle)\n",
    "\n",
    "    with open(model_path + '/encoder.pkl','rb') as handle:\n",
    "        encoder = pickle.load(handle)\n",
    "\n",
    "# Called when a request is received\n",
    "def run(data):\n",
    "    try:\n",
    "        # Pick out the text property of the JSON request.\n",
    "        # This expects a request in the form of {\"text\": \"some text to classify\"}\n",
    "        data = json.loads(data)\n",
    "        text = preprocess_text(data['text'])\n",
    "        vector = vectoriser.transform([text]).toarray()\n",
    "        prediction = model.predict([vector])\n",
    "        prediction = np.round(prediction[0])\n",
    "        prediction = encoder.inverse_transform(np.where(prediction == 1))\n",
    "        return prediction[0]\n",
    "    except Exception as e:\n",
    "        error = str(e)\n",
    "        return error\n",
    "\n",
    "def preprocess_text(text):\n",
    "    processed = re.sub('[^a-zA-Z]', ' ', text)\n",
    "    processed = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', processed)\n",
    "    processed = re.sub(r'\\s+', ' ', processed)\n",
    "\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.environment import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "# Create the environment\n",
    "deployment_env = Environment('document-classification-deployment')\n",
    "\n",
    "# to install required packages\n",
    "conda_packages = ['numpy', 'scikit-learn']\n",
    "pip_packages = ['azureml-dataprep[pandas,fuse]', 'azureml-defaults', 'tensorflow', 'keras']\n",
    "\n",
    "conda_dep = CondaDependencies.create(pip_packages=pip_packages, conda_packages=conda_packages)\n",
    "\n",
    "# Adds dependencies to PythonSection of myenv\n",
    "deployment_env.python.conda_dependencies=conda_dep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running....................................................\n",
      "Succeeded\n",
      "ACI service creation operation finished, operation \"Succeeded\"\n",
      "Healthy\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Model\n",
    "from azureml.core.webservice import AciWebservice\n",
    "from azureml.core.model import InferenceConfig\n",
    "\n",
    "model = ws.models['document_classification_model']\n",
    "\n",
    "inference_config = InferenceConfig(source_directory=service_folder, \n",
    "                                   entry_script='score.py',\n",
    "                                   environment=deployment_env)\n",
    "\n",
    "deployment_config = AciWebservice.deploy_configuration(cpu_cores = 1, memory_gb = 1)\n",
    "\n",
    "service_name = \"document-classification-service\"\n",
    "\n",
    "service = Model.deploy(ws, service_name, [model], inference_config, deployment_config)\n",
    "\n",
    "service.wait_for_deployment(True)\n",
    "print(service.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "0:00:00.097396\n",
      "resume\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "scoring_uri = '<scoring_uri>'\n",
    "headers = {'Content-Type':'application/json'}\n",
    "\n",
    "test_data = json.dumps({'text': 'AMY PROFILE Fund accountant with nearly 2 years of experience in hedge fund administration, which includes preparation of NAV calculations, financial statements and associated reports. Consistently meeting deadlines while ensuring a high quality of work standards. Fast learner, driven for results and analytical in problem solving. WORK EXPERIENCE Citco Fund Services (Singapore) Pte Ltd  Jan 2016 – Present Fund Accountant Calculation of estimate and final NAVs on a daily, weekly and monthly basis Preparation of cash and position reconciliation reports  Daily pre-production tasks such as price checks, interest accruals and fees booking  Maintaining day-to-day relationships with investment managers, brokers and auditors  Communicating with the reconciliations team to ensure consistent and high-quality standards when delivering NAV packages  Undertaking fund migrations from Citco Toronto and Citco Dublin Office  Familiar with various pricing valuation models e.g. Independent Price Verification, External Valuer  Investigation and resolution of breaks  Training of new fund accountants in the team  Assist in reviewing estimate and final NAV packs Norgas Carriers Private Limited Nov 2014 – Apr 2015 Accountant  Processing invoices from suppliers and scheduling payment remittances  Assisted in collecting outstanding debts from debtors  Involved in the year end closing of accounts  Liaised with external auditors during interim and final audit  Monthly bank reconciliation  Perform intercompany journal entries and reconciliation Singapore Armed Forces Jul 2013 – July 2014 Operation Specialist/ NSF Battery Sergeant Major (2nd Sergeant, 24th Battalion Singapore Artillery)  In charge of the training management and administration matters, ensuring smooth running and operation of the battalion  Responsible for the welfare, discipline and regimentation of 60 fellow NSFs  Rallied with superiors to keep a lookout for troubled soldiers, acted as the first level of feedback for peers EDUCATION AND QUALIFICATIONS Royal Melbourne Institute of Technology (RMIT) Jul 2014 – Oct 2015 Bachelor of Business (Accountancy) GPA: 3.1 / 4 Temasek Polytechnic Apr 2009 – Apr 2012 Diploma in Accounting and Finance Ngee Ann Secondary School Jan 2005 – Nov 2008 GCE ‘O’ Levels CertificateSKILLS AND COMPETENCIES  Trained in accounting softwares, Aexeo, Agresso Business World, Sage AccPac and MYOB  Proficient in Microsoft Excel, PowerPoint and Word  Competent in Financial and Economic Databases (Bloomberg, Thomson Reuters)  Languages spoken: English, Chinese, Cantonese  Languages written: English, Chinese'})\n",
    "\n",
    "response = requests.post(scoring_uri, data=test_data, headers=headers)\n",
    "\n",
    "print(response.status_code)\n",
    "print(response.elapsed)\n",
    "print(response.json())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
